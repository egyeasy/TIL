# 190620 빅데이터 도메인 교육

#### 빅데이터 분석과 병렬 분산 처리 알고리즘

Scale-out : 아주 많 은 값싼 컴퓨터들을 이용

Scale-up : 적은 수의 좋은 컴퓨터



#### 맵리듀스(MapReduce)

대규모로 컴퓨팅을 잘 분산 병렬로 처리할 수 있도록 해준 첫 번째 프로그래밍 모델

병렬 분산 알고리즘을 쉽게 개발할 수 있게 도와주는 모델

-> 프로그래머가 low-level을 잘 알지 못해도 function 몇 가지로 병렬 처리 구현 가능



#### Hadoop

오픈 소스 커뮤니티에서 맵리듀스를 구현한 프레임워크 -> Hadoop -> 유저가 map, reduce, main 함수들만 프로그래밍 해주면 됨





## 프로젝트 설명

#### 추천 시스템

어느 기업이나 추천 시스템은 아주 중요한 부분이다(Netflix, Youtube)

- 클러스터링 알고리즘
- 협업 필터링(Collaborative filtering)의 알고리즘을 Python으로 구현



#### 프로젝트 2 목표

- Django를 이용하여 실제 어플리케이션 UI 구현

- numpy, scipy 라이브러리 -> 행렬 표현 및 연산 학습

  Sparse matrix -> numpy와 scipy 연산을 활용해 효율적인 연산

- MovieLens 영화 데이터의 평점 행렬 -> 각 유저에 대해 유사한 유저가 본 영화를 추천

- 클러스터링 알고리즘의 이해 및 python 구현



#### 프로젝트 3 목표

- 추천에 이용되는 Collaborate Filtering 주요 알고리즘 이해

  - KNN
  - Matrix factorization
  - Matrix factorization + PLSI
  - Matrix factorization + LDA

- Probabilistic Modeling

- 영화 평점 외에 영화에 대한 다른 정보도 이용하는 알고리즘(영화 줄거리, 소셜 네트워크 등)

- Python 이용 빅데이터 효율 처리 코딩 실습

  broadcasting 기법, sparse matrix format



#### 개발 환경 구성(Windows)

- `pip install numpy`, `pip install scipy`



#### Cluster Data

웹페이지에서 clustering 클릭 -> 클러스터링 알고리즘 파이썬 파일 실행 -> 결과를 DB에 저장 -> DB에서 결과 가져와서 보여주기



#### Train Model

collaborative filtering을 통해 유저에게 추천할 만한 영화를 추출

유저가 본 영화들도 리스트업



#### Recommend

추천 몇 개 할 지 정하기



=> 유저에게 영화를 추천하기 위한 알고리즘을 사용할 수 있는 관리자 페이지라고 보면 될 듯



#### 프로젝트 도전

- UI 디자인

- 영화 포스터, 예고편, 동영상, 리뷰 등을 추천할 때에 같이 보여주기

- 멀티 코어, GPU 더욱 잘 이용할 수 있는 python 코드

- 기계학습 툴 -> PLSI 대신에 딥러닝 알고리즘 접목

- 하둡 MapReduce 프레임워크 이용 -> 병렬 분산 3가지 알고리즘 구현(Python에서도 가능)

- 더 복잡한 실제 데이터 이용 추천 시스템 개발

  Twitter에서 유저들에게 관심이 가는 Tweet message나 follow할 사람들을 추천



#### Twitobi: A recommendation system for twitter -> PLSI보다 한 단계 더 복잡한 모델

사람들은 자신이 팔로우한 사람의 관심사에 의해서도 영향을 받는다.

=> 유저가 팔로우하지 않았지만 팔로우할 만한 유저 / 트윗을 추천

논문 발표에 대한 얘기를 하던 사람이, 엠마 왓슨을 팔로우 하고 있고, 엠마 왓슨이 쓴 트위터에 대한 반응으로 트윗을 쓸 수 있다.

자기 전공에 대해 쓸 확률 : a

해리 포터에 대해 쓸 확률 : 1 - a -> 엠마왓슨을 팔로우하고 있으므로 해리포터 또한 가능한 주제로 고려하는 것

PLSI 식에 p(f|u)를 추가한 것.





## 프로젝트 관련 기초지식

### Clustering이란?

7개의 점이 있다면 (2^7) / 2가지 경우의 clustering 결과가 가능하다.

모델에 대한 평가 - 중심점과 각 점의 거리의 총합이 작을수록 clustering이 잘 된 것이라 볼 수 있다.

ex) square-error criterion





### K-Means Clustering

#### 기본 개념

1. 각각의 점들을 랜덤하게 클러스터에 할당
2. 각각의 중앙값 구하기
3. 각 점과 가장 가까운 중앙값의 클러스터로 그 점을 옮기기
4. 클러스터의 중앙값을 새로 구함
5. 또 다시 3 -> 4
6. 클러스터가 더이상 변하지 않을 때 종료



#### 단점(drawbacks)

- 클러스터별 점들의 개수가 불균형 -> 밀도가 낮은 클러스터를 여러 개로 분리해버릴 수 있는 등의 문제
- 거리를 이용해서 함 -> 구형의 결과가 나옴 -> but 구형이 아닌 형태로 cluster가 존재한다면?
- sensitive to outliers -> outlier에 의해 bias가 생기게 됨
- K-Medoids : 중심과 가장 가까운 점을 기준으로 삼는 방법





### EM Clustering

generative model(생성 모델)

hidden parameters(공의 개수를 알 수 없는 주머니) -> given obervable data(꺼내어진 공의 분포)

후자로부터 전자를 infer하는 것

하나의 특정한 주머니 set에서 지금의 관측결과가 도출될 확률을 구할 수 있다.

빨간 공이 나올 확률을 편미분하여 0이 되는 지점 -> 확률 극대화 optimization

미분을 편하게 하기 위해 : 미분 -> 파라미터 얻고 -> 미분 -> ...

한 값을 업데이트하고 다른 값을 업데이트하는 식

#### Gaussian mixture: the weighted sum of k Gaussian probability distributions

each gaussian probability distribution represents a cluster

k개의 가우시안 분포가 있을 때, 각각의 점들이 특정 분포에 속할 때의 확률을 계산 -> 확률이 가장 높은 cluster 분포를 찾는다.

x1이 나올 확률 = (클러스터1이 나올 확률) * (클러스터1에서 x1이 나올 확률) + ... = 총합

x2가 나올 확률 = 총합

...

----------------------------------------

위의 확률들을 모두 곱하면 이 데이터 분포가 만들어졌을 확률을 구한다.

이 확률이 가장 높아지는 지점을 미분을 통해 optimzation -> 파이(클러스터 나올 확률), 뮤(클러스터 평균), 시그마(표준편차) 구하기 -> log likelihood 사용

cj가 뭐지? -> x가 그 클러스터에서 나왔을 확률. 이걸 쓰지 않으면 파이가 파이에 의한 식으로 표현되어 자기가 참조됨 -> optimization update가 불가능. 매개해주는 변수로 c를 사용.

E-step -> compute p(cj|xj) for every j=1, ..., k and every xj

M-step -> compute 파이, 뮤, 시그마 for every

E-step과 M-step 사이를 반복하며 구한 값들을 계속해서 업데이트 -> for문이나 while문 속에서 업데이트 되는 정도가 극히 작아지면 break





### PLSI(Probabilistic Latent Semantic Indexing) Model

A generative model

각각의 단어를 하나의 주제에서 뽑아낼 것. 각각의 토픽마다 뽑아지는 단어의 확률이 다 다름(이중 구조)

Obama라는 주제에 대해 단어가 나올 확률의 분포가 있음 -> 그곳에서 계속해서 뽑아서 글을 쓴다 -> "Barack Galaxy Excellent Galaxy Design Good Performance"



문서를 고를 확률 P(di)

문서에 대해 특정 토픽이 골라질 확률 P(zk | dj)

토픽에 대해 단어가 골라질 확률 P(wv|zk)



이것들을 EM clustering을 통해 확률들을 업데이트할 것

z : unobservable



각각의 확률들을 미분하여 optimization -> 특정한 주제로 확률이 극대화 수렴하게 됨



문장이 나올 확률을 계산하여 이 글을 쓸 확률이 얼마인지를 구함.

영화 줄거리마다 주제의 분포를 구한다 -> 각각의 영화 줄거리마다 어떤 주제에 가까운지를 알 수 있다.

실제로 추천 시스템에 PLSI를 넣었을 때 효과가 있다.





### Matrix Factorization

#### collaborative filtering method

1. Memory based method

   과거의 rating을 이용

2. model based method

   유저마다 벡터가 있다 -> 실제 데이터를 잘 설명하는 유저와 아이템 벡터들의 파라미터를 구하기



#### Memory based method: user-user similarity

고객별 구매 내역에서 - > 상품별로 함께 구매된 정도를 2차원 매트릭스로 나타낸다(c1과 c1의 유사도 = 1, c1과 c2의 유사도= 0.82)

이 유사도 matrix(n x n)와 구매내역 matrix(n x m)을 곱함 -> n x m의 매트릭스 : 해당 고객에게 해당 product를 추천해줘도 될지 정도를 나타냄



#### matrix factorization

특정한 유저의 벡터(user latent vector)와 아이템 벡터(item latent vector)를 곱한(내적한) 것이 현재의 결과를 나타낸다면, 해당 유저 벡터와 아이템 벡터를 optimize 한다.



#### improving matrix factorization

PLSI와 합하여 라그랑지 식을 구성한다.





### project 3 - KNN 알고리즘

user입장에서도, item 입장에서도 KNN이 가능하다.

데이터가 너무 sparse하면 neighbor를 찾을 수 없는 경우도 있음 -> 데이터가 적당히 커야함



R_u1_i1 : user1이 item1에 대해 rating

**user item latent matrix**
	p1 p2 p3 p4 p5

u1 4 3 0 2 1

u2 0 3 4 0 5

u3 1 0 2 0 1



u1, u2 내적 -> 14

S_u1_u2 = 14 / sqrt(30*40) : u1과 u2의 similarity

S_u1_u3 = 5/sqrt(30*6)

R_u1_i3을 구하려고 해보면

(S_u1_u2 * R_u2_i3 + S_u1_u3 * R_u3_i3) / (S_u1_u2 + S_u1_u3) -> 평점들의 가중 평균을 구한다.





### project 3 - Matrix Factorization 알고리즘

향상된 matrix factorization - 아이템 벡터를 구하는 데에 PLSI 주제 분석 알고리즘을 통합하여 최적화

















